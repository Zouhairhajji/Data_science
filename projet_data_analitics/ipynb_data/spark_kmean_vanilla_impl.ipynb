{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"SPARK_HOME\"] = \"/Users/zouhairhajji/Documents/dev/spark-2.4.0-bin-hadoop2.7\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import csv\n",
    "import numpy as np\n",
    "import random\n",
    "from math import sqrt\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "        .master('local[*]')  \\\n",
    "        .enableHiveSupport() \\\n",
    "        .getOrCreate()\n",
    "\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, [5.7, 2.5, 5.0, 2.0]),\n",
       " (1, [5.1, 3.7, 1.5, 0.4]),\n",
       " (2, [5.5, 2.3, 4.0, 1.3])]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "def loadData(namefile) :\n",
    "    textfile = sc.textFile(namefile)    \\\n",
    "            .zipWithIndex()   \\\n",
    "            .map(lambda x: (x[1], x[0].split(',')  ))  \\\n",
    "            .map(lambda x: (x[0], [float(i) for i in x[1]]  ))\n",
    "            \n",
    "    return textfile\n",
    "\n",
    "def dimension(rdd_load_data) :\n",
    "    return rdd_load_data.map(lambda x: len(x[1])).reduce(lambda x, y: max(x, y)) \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def closestcentroide(list1):\n",
    "    cluster = list1[0][0]\n",
    "    min_dist = list1[0][1]\n",
    "    for elem in list1:\n",
    "        if elem[1] < min_dist:\n",
    "            cluster  = elem[0]\n",
    "            min_dist = elem[1]\n",
    "    return (cluster, min_dist)\n",
    "\n",
    "def computeCentroids(rdd,findcentroid):\n",
    "\n",
    "    union = findcentroid.join(rdd)\n",
    "    centroid_point = union.map(lambda x: (x[1][0][0],x[1][1][:-1])) \n",
    "    numpoints = centroid_point.map(lambda x: (x[0],1)).reduceByKey(lambda x,y: x+y) \n",
    "    sum = centroid_point.reduceByKey(lambda x,y: [x[i]+y[i] for i in range(len(x))]) \n",
    "    union2 = sum.join(numpoints)\n",
    "    newcentroids = union2.map(lambda x : (x[0], mean(x[1][0],x[1][1]))) \n",
    "\n",
    "    return newcentroids  \n",
    "\n",
    "\n",
    "def calculate_distance(x, y):\n",
    "    s = sqrt(sum([ (a - b)**2 for a,b in zip(x,y)]))\n",
    "    dis = float('%.3f'%(s))\n",
    "    return dis\n",
    "\n",
    "\n",
    "\n",
    "def initCentroids(rdd_flat_data, k):\n",
    "    seed = random.randint(0, 100)\n",
    "    \n",
    "    centroids = rdd_flat_data.takeSample(False, k, seed=seed)\n",
    "    return  sc.parallelize(centroids)   \\\n",
    "                .zipWithIndex()         \\\n",
    "                .map(lambda x: (x[1], x[0][1])) \n",
    "\n",
    "    \n",
    "def computeIntraClusterDistance(rdd, centroids):\n",
    "    initdistance = assignToCluster(rdd,centroids) \n",
    "    totaldistance = initdistance.map(lambda x: (1,x[1][1])).reduceByKey(lambda x,y: x+y)\n",
    "    return totaldistance\n",
    "    \n",
    "def mean(x,n):\n",
    "    return [x[i]/n for i in range(len(x))]\n",
    "\n",
    "flat_data = loadData('iris_input.data')\n",
    " \n",
    "initCentroids(flat_data, 3).collect()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running step  1\n",
      "    ->   3.0925925698730383\n",
      "running step  2\n",
      "    ->   0.9849446161390345\n",
      "running step  3\n",
      "    ->   0.09082776850515689\n",
      "running step  4\n",
      "    ->   0.050952410538477044\n",
      "running step  5\n",
      "    ->   0.0\n",
      "distance :  [(1, 84.66865903654964)]\n",
      "step_coup :  5\n"
     ]
    }
   ],
   "source": [
    "def calculate_distance(x, y):\n",
    "    x = np.array(x)\n",
    "    y = np.array(y)\n",
    "    return np.sqrt(np.sum(np.square( x-y )))  \n",
    "    \n",
    "    \n",
    "def assignToCluster(rdd, centroids):\n",
    "    cartes = rdd.cartesian(centroids) \n",
    "    \n",
    "    cartes = cartes.map(lambda x: (x[0][0], (x[1][0], calculate_distance(x[1][1], x[0][1]))  ) )\n",
    "    cartes = cartes.groupByKey().mapValues(closestcentroide)\n",
    "    return cartes\n",
    "\n",
    "\n",
    "def closestcentroide(y):\n",
    "    y = list(y)\n",
    "    closest = y[0]\n",
    "    \n",
    "    for couple in y:\n",
    "        if(closest[1] > couple[1]): \n",
    "            closest = couple\n",
    "            \n",
    "    return  closest\n",
    "\n",
    "def calculate_new_coordinate(x): \n",
    "    mean = lambda x: sum(x) / len(x)\n",
    "    \n",
    "    return [ round(mean(x), 2) for x in zip(*x)]  \n",
    "\n",
    "def computeCentroids(rdd, assigned_centroids): \n",
    "     return assigned_centroids   \\\n",
    "                .join(rdd)       \\\n",
    "                .map(lambda x: (x[1][0][0], x[1][1]) )       \\\n",
    "                .groupByKey()       \\\n",
    "                .mapValues(calculate_new_coordinate)\n",
    "\n",
    "def computeIntraClusterDistance(rdd, centroids):\n",
    "    initdistance = assignToCluster(rdd, centroids) \n",
    "    totaldistance = initdistance.map(lambda x: (1, x[1][1]) ).reduceByKey(lambda x,y: x+y)\n",
    "    return totaldistance   \n",
    "\n",
    "def kmeans(filename, k, m, d):\n",
    "    flat_data = loadData(filename)\n",
    "    dim = dimension(flat_data)\n",
    "    \n",
    "    centroids = initCentroids(flat_data, k)\n",
    "    \n",
    "    \n",
    "    \n",
    "    #assign every point to a centroids\n",
    "    assigned_points = assignToCluster(flat_data, centroids)\n",
    "    \n",
    "    #compute centroids\n",
    "    old_centroids = computeCentroids(flat_data, assigned_points)\n",
    "    \n",
    "    converged = False\n",
    "    step_count = 0\n",
    "    step_max = 10\n",
    "    \n",
    "    while not converged  and step_count < step_max:\n",
    "        step_count += 1\n",
    "        print('running step ', step_count)\n",
    "        \n",
    "        \n",
    "        assigned_points = assignToCluster(flat_data, old_centroids) \n",
    "        \n",
    "        new_centroids   = computeCentroids(flat_data, assigned_points)\n",
    "        distance_intracluster = computeIntraClusterDistance(flat_data, new_centroids)\n",
    "        \n",
    "        \n",
    "        centroid_changed = old_centroids.join(new_centroids) \\\n",
    "            .map(lambda x: np.sqrt(np.sum(np.square(np.array(x[1][0]) - np.array(x[1][1])))) ) \\\n",
    "            .reduce(lambda x, y: x+y) \n",
    "        \n",
    "        print('    ->  ' ,  centroid_changed)\n",
    "        \n",
    "        \n",
    "         \n",
    "        if centroid_changed == 0:\n",
    "            converged = True \n",
    "        old_centroids = sc.parallelize(new_centroids.collect()) \n",
    "            \n",
    "        \n",
    "    return (step_count, distance_intracluster.take(1), assigned_points, old_centroids  ) \n",
    "     \n",
    "    \n",
    "\n",
    "step_count, distance, points, centroids= kmeans('iris_input.data', 4, 4, 10)\n",
    "print(\"distance : \" , distance)\n",
    "print(\"step_coup : \" , step_count)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 47), (1, 23), (2, 50), (3, 30)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "points.map(lambda x: (x[1][0], x[0]) ).groupByKey().mapValues(lambda x: len(list(x))).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  ml lib\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from pyspark.ml.clustering import KMeans\n",
    "from pyspark.ml.evaluation import ClusteringEvaluator\n",
    "from pyspark.ml.feature import VectorAssembler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kmeans_mllib(filename, k):\n",
    "    flat_data = loadData(filename).map(lambda x: x[1])  \n",
    "    \n",
    "    features = [\"c1\", \"c2\", \"c3\", \"c4\"]\n",
    "    dataset = spark.createDataFrame(flat_data, features)\n",
    "\n",
    "     \n",
    "    \n",
    "    transformed_dataset = VectorAssembler(inputCols=features, outputCol=\"features\").transform(dataset) \n",
    "    \n",
    "    \n",
    "    kmeans = KMeans(k=k, seed=1) \n",
    "    model = kmeans.fit(transformed_dataset.select('features'))\n",
    "    \n",
    "    return model.transform(transformed_dataset) \n",
    "    \n",
    "kmeans_mllib = kmeans_mllib('iris_input.data', 4)\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+\n",
      "|prediction|count|\n",
      "+----------+-----+\n",
      "|         1|   23|\n",
      "|         3|   30|\n",
      "|         2|   47|\n",
      "|         0|   50|\n",
      "+----------+-----+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(0, 47), (1, 23), (2, 50), (3, 30)]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kmeans_mllib.groupBy('prediction').count().show()\n",
    "\n",
    "points.map(lambda x: (x[1][0], x[0]) ).groupByKey().mapValues(lambda x: len(list(x))).collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
